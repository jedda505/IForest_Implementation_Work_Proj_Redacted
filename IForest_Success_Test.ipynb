{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This document will compare the model report to the audit reports carried out by case-workers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing packages and read in CSVs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "audit = pd.read_excel('s3://xxxxxxxxx/Audit.xlsx' )\n",
    "\n",
    "mdl_SENSITIVE = pd.read_csv('s3://xxxxxxxxx/mdl_SENSITIVE.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(audit), len(mdl_SENSITIVE))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove any entries that weren't put through the model\n",
    "\n",
    "mdl_SENSITIVE = mdl_SENSITIVE[mdl_SENSITIVE['MODEL_PREDICTION'].notna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(mdl_SENSITIVE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Take rows with the lowest model prediction e.g. -1 outliers and sort so those are at the top\n",
    "\n",
    "mdl_SENSITIVE = mdl_SENSITIVE.sort_values(by= ['REF_NUM','MODEL_PREDICTION'], ascending=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop duplicates in the audit file\n",
    "\n",
    "audit = audit.loc[audit['REF_NUM'].notnull()].drop_duplicates('REF_NUM')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "audit['REF_NUM'] = audit['REF_NUM'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for cols in mdl_SENSITIVE:\n",
    "    print(cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(audit['REF_NUM']), len(mdl_SENSITIVE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's merge to get a comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mdl_audit_SENSITIVE = mdl_SENSITIVE.merge(audit, how='outer', on=['REF_NUM'], indicator=True)\n",
    "mdl_audit_SENSITIVE = mdl_audit_SENSITIVE.dropna(axis=1, how='all')\n",
    "len(mdl_audit_SENSITIVE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def agg_doc(df):\n",
    "    merge_status = df['_merge'].unique()\n",
    "    root_cause = df['Root Cause of Feedback'].unique()\n",
    "    doc_score = df['SKLEARN_SCORE_ANOMALIES'].mean()\n",
    "    '''If any line on doc ==-1 (outlier) then mark whole doc as outlier'''\n",
    "    if (df['MODEL_PREDICTION']==-1).any():\n",
    "#     if (len(df[df['MODEL_PREDICTION']==-1])>=2):\n",
    "\n",
    "        return pd.DataFrame({'outlier_value': ['Outlier'], '_merge': merge_status, 'root_cause': root_cause,\n",
    "                            'overall_doc_outlier_score': doc_score})\n",
    "    \n",
    "#     elif (len(df)==1) and ((df['MODEL_PREDICTION']==-1).any()):\n",
    "        \n",
    "#         return pd.DataFrame({'outlier_value': ['Outlier'], '_merge': merge_status, 'root_cause': root_cause})\n",
    "    \n",
    "    else:\n",
    "\n",
    "        return pd.DataFrame({'outlier_value': ['Non_Outlier'], '_merge': merge_status, 'root_cause': root_cause})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_output = mdl_audit_SENSITIVE.groupby('REF_NUM').apply(agg_doc)\n",
    "\n",
    "grouped_output = pd.DataFrame(grouped_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(grouped_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adding a column to state whether overall doc has any lines that are detected as an outlier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_output.to_csv('s3://alpha-app-laa-fas-qc-dsa/mdl_audit_sensitive(lines grouped).csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mdl_audit_SENSITIVE.to_csv('s3://alpha-app-laa-fas-qc-dsa/mdl_audit_sensitive.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fasvenv",
   "language": "python",
   "name": "fasvenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
