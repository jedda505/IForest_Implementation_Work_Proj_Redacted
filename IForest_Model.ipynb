{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import boto3\n",
    "import io\n",
    "from sklearn import preprocessing\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
    "from sklearn import model_selection\n",
    "import shap\n",
    "\n",
    "np.random.seed(55)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def create_column_lists(df, drop_columns):\n",
    "    \"\"\"Function to create column lists and return a dictionary of columns.\n",
    "    A dictionary is a list of what are know as key-value pairs formatted like\n",
    "    this {'key':value}\n",
    "    \n",
    "    Returns the following columms:\n",
    "        'all_columns', 'keep_columns', 'date_columns_keep',\n",
    "        'drop_columns', 'cat_columns', 'numeric_columns'\n",
    "    \n",
    "    Args:\n",
    "        df (pd.DataFrame): the data frame to find columns\n",
    "        drop_columns (list): list of column names to be excluded from future analysis\n",
    "        \n",
    "    Returns:\n",
    "        dict: dictionary of column name types\n",
    "    \"\"\"\n",
    "    # -- all_columns is a list of all the column names in the dateframe\n",
    "    all_columns = df.columns.tolist()\n",
    "    \n",
    "    # -- keep_columns is any column from all_columns that does not feature in\n",
    "    # -- in the drop columns list\n",
    "    keep_columns = [c for c in all_columns if c not in drop_columns]\n",
    "    \n",
    "    # -- Seperating date columns. These need to be handled differently by the model\n",
    "    date_columns_keep = [col for col in df[keep_columns].columns if 'DATE' in col]\n",
    "    \n",
    "    # -- Make empty list for categoric cols and numeric cols to populate\n",
    "    catcols = []\n",
    "    numcols = []\n",
    "    for colcls in df[keep_columns].columns:\n",
    "        if df[colcls].dtype == 'O':\n",
    "            catcols.append(colcls)\n",
    "        else:\n",
    "            numcols.append(colcls)\n",
    "            \n",
    "    # -- remove unique identifying column from analysis\n",
    "    if 'unique_id' in numcols:\n",
    "        numcols.remove('unique_id')\n",
    "        \n",
    "    column_dict = {'all_columns': all_columns, 'keep_columns': keep_columns, 'date_columns_keep': date_columns_keep,\n",
    "                   'drop_columns': drop_columns, 'cat_columns': catcols, 'num_columns': numcols}\n",
    "    \n",
    "    return column_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_date(df, date_columns):\n",
    "    \"\"\"\n",
    "    Transform the date columns to weekday: a number 0-7\n",
    "    \n",
    "    Args:\n",
    "        df (pd.DataFrame): the dataframe to transform\n",
    "        date_columns (list): list of column names of date columns\n",
    "        \n",
    "    Returns:\n",
    "        pd.DataFrame: the dataframe with date columns tranformed\n",
    "    \"\"\"\n",
    "    # --turning date columns into datetime format for python to read\n",
    "    df[date_columns] = df[date_columns].apply(lambda x: pd.to_datetime(x, yearfirst=True, infer_datetime_format=True))\n",
    "\n",
    "    # --convert to dates to weekday for model processing\n",
    "    for column in date_columns:\n",
    "        df[column] = df[column].dt.weekday # day_name would return name of day rather than integer\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model_input_array_one_hot_encoding(df, catcols, numcols):\n",
    "    \"\"\"Function to create the input array for the use in the model.\n",
    "    This function uses one not encoding on the categorical columns\n",
    "    and then stacks these with the numerical columns to create the array.\n",
    "    Also returns the one hot encoder object to use with de-encoding \n",
    "    \n",
    "    Args: \n",
    "        df (pd.DataFrame): the dataframe to create array from\n",
    "        catcols (list): list of categorical columns to encode\n",
    "        numcols (list): list of numerical columns to encode\n",
    "        \n",
    "    Returns:\n",
    "        np.array, preprocessing.OneHotEncoder(): array to pass to model, the one hot encoder \n",
    "    \n",
    "    \"\"\"\n",
    "    # -- OneHot Encoder\n",
    "    oneHot = preprocessing.OneHotEncoder()\n",
    "    oneHot.fit(df[catcols])\n",
    "    transformed = oneHot.transform(df[catcols])\n",
    "\n",
    "    # -- Convert to array so it can be read by the ML model\n",
    "    dense_transformed = transformed.todense()\n",
    "    \n",
    "    array_to_go_to_model = np.array(np.hstack((dense_transformed, df[numcols].to_numpy())))\n",
    "    \n",
    "    return array_to_go_to_model, oneHot\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def label_encoder_for_columns(df, catcols):\n",
    "    \"\"\"Function to use label encoder for columns. \n",
    "    The function returns a dictionary of label encoders for \n",
    "    all the categorical columns along with the transformed dataframe\n",
    "    \n",
    "    Args:\n",
    "        df (pd.DataFrame): the dataframe to create array from\n",
    "        catcols (list): list of categorical columns to encode\n",
    "    \n",
    "    Returns:\n",
    "        dict, pd.DataFrame: Dictionary of label encoders\n",
    "    \"\"\"\n",
    "    encoder_dict = {}\n",
    "    for colcls in catcols:\n",
    "        le = preprocessing.LabelEncoder()\n",
    "        le.fit(lnsraw[colcls])\n",
    "        df[colcls] = le.transform(df[colcls])\n",
    "        encoder_dict[colcls] = le\n",
    "        \n",
    "    return encoder_dict, dfc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --Set random state IForest\n",
    "\n",
    "Iso_Forest = IsolationForest(random_state= 55)\n",
    "\n",
    "# --Create parameter finder for IForest\n",
    "\n",
    "ParamGrid = {'n_estimators': list(range(100, 800, 5)), \n",
    "              'max_samples': list(range(100, 500, 5)), \n",
    "              'max_features': [0.1,0.25,0.5,0.75, 0.9,1.00], \n",
    "              'bootstrap': [True, False]}\n",
    "\n",
    "# -- Defining Scorer for GridSearchCV Parameter Finder\n",
    "\n",
    "def scorer_f(estimator, array_to_go_to_model):\n",
    "    return np.mean(estimator.score_samples(array_to_go_to_model))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read in CSVs including inferring the date format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lnsraw = pd.read_csv('xxxxxxxxxxxxxx/Pseud_lns.csv', \n",
    "                     parse_dates=True, \n",
    "                     infer_datetime_format=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Filter to only include paid or rejected bills - none that are under assessment. I also am removing any bills that are associated with 0 assessed claim total as there is no risk associated with these."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lnsraw = lnsraw[(lnsraw['I_STATUS'] == 'P') | (lnsraw['I_STATUS'] =='R')]\n",
    "\n",
    "lnsraw = lnsraw[(lnsraw['AC_TOTALS'] != 0)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use this section to select matter type - be sure to rename the output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lnsraw.groupby('M_NAME').size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lnsraw = lnsraw[(lnsraw['M_NAME'] == 'SCX')]\n",
    "\n",
    "# Remove duplicates\n",
    "lnsraw = lnsraw.drop_duplicates()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### List of columns to drop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_columns = lnsraw[['U_DATE', 'SR_INC','CAT_CODE', \n",
    "                      'CAT_NAME', 'M_CODE', 'B_TASK_TYPE', \n",
    "                      'D_TASK_TYPE','BS_DATE',\n",
    "                    'CP_ID', 'SRI_WEEK_COMMENCING', 'DR_DATE', \n",
    "                    'B_TYPE', 'DS_DATE', 'BUA_DATE', \n",
    "                    'DEC', 'LOC', 'DT_STATUS', 'A_NUM_PSEUD',\n",
    "                    'REF_NUM_PSEUD', 'CSL_NAME_PSEUD', 'C_NAME_PSEUD',\n",
    "                    'I_ANUM_PSEUD', 'I_STATUS', 'BT_STATUS',\n",
    "                    'AC_TOTAL','unique_id' \n",
    "                          # Remove M Name if not Analysing all matter types\n",
    "                          ,'M_NAME'\n",
    "                         ]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please note that LOC was removed due to null values - not sure what these represent. DT_Status was removed as it \n",
    "was found that it had no impact on the model.\n",
    "\n",
    "###really important: M_NAME is there for when we exclude this and feed the model with only one M type"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # Use the create_column_lists function to identify the columns that are of different types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_dict = create_column_lists(lnsraw, drop_columns)\n",
    "\n",
    "lnsraw = process_date(lnsraw, column_dict['date_columns_keep'])\n",
    "\n",
    "\n",
    "# Remove NA from numerical values before encoding\n",
    "lnsraw[column_dict['num_columns']] = lnsraw[column_dict['num_columns']].fillna(0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Show list of columns going into model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_dict['cat_columns'], column_dict['num_columns']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_dict['keep_columns']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoding\n",
    "\n",
    "Using the functions from above to encode the columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#le_dict,lnsraw =  label_encoder_for_columns(lnsraw, column_dict['catcols'])\n",
    "#frame_to_go_to_model = lnsraw[column_dict['keep_columns']]\n",
    "\n",
    "array_to_go_to_model, oneHot = create_model_input_array_one_hot_encoding(lnsraw, column_dict['cat_columns'],\n",
    "                                                                         column_dict['num_columns'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training the I-Forest Model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting up IForest Model and parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_optimiser = RandomizedSearchCV(IsolationForest(), ParamGrid,scoring=scorer_f, n_iter=80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IFor_params = param_optimiser.fit(array_to_go_to_model)\n",
    "print(IFor_params.best_params_) \n",
    "# -- first attempt: {'n_estimators': 120, 'max_samples': 285, 'max_features': 0.75, 'bootstrap': True}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IFor = IsolationForest(n_estimators=120, max_samples=285, max_features=1.0, bootstrap=False, contamination=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IFor.fit(array_to_go_to_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = IFor.predict(array_to_go_to_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explore the predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating a dataframe to translate each feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "encoded_columns = oneHot.get_feature_names_out()\n",
    "\n",
    "model_feature_names = np.concatenate([encoded_columns, np.array(column_dict['num_columns'])])\n",
    "\n",
    "df_to_model = pd.DataFrame(array_to_go_to_model, columns = model_feature_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using SHAP to explain/evaluate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "explainer = shap.Explainer(IFor)\n",
    "shap_values = explainer(df_to_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize the first prediction's explanation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "shap.summary_plot(shap_values, df_to_model, plot_type=\"bar\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Translator for Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_features = oneHot.get_feature_names_out()\n",
    "\n",
    "pd.DataFrame(list_of_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Put the predictions in to the original dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lnsraw['MODEL_PREDICTION'] = y_pred\n",
    "\n",
    "lnsraw['SKLEARN_SCORE_ANOMALIES'] = IFor.decision_function(array_to_go_to_model)\n",
    "\n",
    "lnsraw['ANOMALY_SCORE_ORIG_PAPER'] = [-1*s + 0.5 for s in lnsraw['SKLEARN_SCORE_ANOMALIES']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lnsraw"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Look at outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(lnsraw[column_dict['keep_columns']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lnsraw['IA_NUM_PSEUD'].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lnsraw['I_STATUS'].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lnsraw['I_STATUS'].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lnsraw['ACTIVITY'].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lnsraw.to_csv('xxxxxxxxxxxxxxxxxx/mdl.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fasvenv",
   "language": "python",
   "name": "fasvenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
